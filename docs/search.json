[
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "test = 1\nprint(test)\nstr = \"Geir får det ikke til\"\nprint(str)\n\n1\nGeir får det ikke til"
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n2",
    "crumbs": [
      "Reference",
      "About"
    ]
  },
  {
    "objectID": "oldexams.html",
    "href": "oldexams.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n2",
    "crumbs": [
      "Old exams",
      "About"
    ]
  },
  {
    "objectID": "module-4.html",
    "href": "module-4.html",
    "title": "Module 4",
    "section": "",
    "text": "Module 4",
    "crumbs": [
      "Modules",
      "Module 4: Hypothesis testing",
      "Module 4"
    ]
  },
  {
    "objectID": "module-2.html",
    "href": "module-2.html",
    "title": "Module 2",
    "section": "",
    "text": "Module 2",
    "crumbs": [
      "Modules",
      "Module 2: Probability",
      "Module 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the website for TECH3 Applied Statistics. We will use this website as a supplement to lectures. The website is an ongoing development, so not all subjects will have content yet. Below you will find a detailed (preliminary) lecture plan, link to the textbook and curriculum.\n\n\n\n\n\nPython Companion to Statistical Thinking in the 21st Century\n\n\n\nAll the material on this website."
  },
  {
    "objectID": "index.html#literature",
    "href": "index.html#literature",
    "title": "Introduction",
    "section": "",
    "text": "Python Companion to Statistical Thinking in the 21st Century"
  },
  {
    "objectID": "index.html#curriculum",
    "href": "index.html#curriculum",
    "title": "Introduction",
    "section": "",
    "text": "All the material on this website."
  },
  {
    "objectID": "datalab-2.html",
    "href": "datalab-2.html",
    "title": "Datalab 2",
    "section": "",
    "text": "Datalab 2",
    "crumbs": [
      "Datalabs",
      "Datalab 2"
    ]
  },
  {
    "objectID": "data/countries/country_data_wrangling.html",
    "href": "data/countries/country_data_wrangling.html",
    "title": "Country data - wrangling",
    "section": "",
    "text": "Wrangling of countries data into a single dataset\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ncountry codes with continent from https://datahub.io/core/country-codes\n\n# north america is coded as NA so need to exclude it from NA list\ncountry_codes &lt;- read_delim('country_codes.csv', na=c('')) %&gt;%\n    rename(CountryCode = `ISO3166-1-Alpha-3`, \n           Name = `UNTERM English Formal`) %&gt;%\n    select(Continent, CountryCode)\n\nRows: 250 Columns: 56\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (48): FIFA, Dial, ISO3166-1-Alpha-3, MARC, is_independent, ISO3166-1-num...\ndbl  (5): Intermediate Region Code, M49, Sub-region Code, Region Code, Geona...\nnum  (2): GAUL, ISO4217-currency_minor_unit\nlgl  (1): Global Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nfrom google, this contains latitude/longitude\n\ncountries &lt;- read_delim('countries/countries.csv') %&gt;%\n  rename(CountryName = name)\n\nRows: 245 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, name\ndbl (2): latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nfrom worldbank, this contains population over time\n\n\nyears are in columns\n\npopulation &lt;- read_csv('worldbank/API_SP.POP.TOTL_DS2_en_csv_v2_3469297.csv', show_col_types = FALSE, skip=4) %&gt;% \n  select(-`...66`) %&gt;%\n  rename(CountryName = `Country Name`,\n         CountryCode = `Country Code`,\n         Population2020 = `2020`) %&gt;%\n  select(CountryName, CountryCode, Population2020)\n\nNew names:\n• `` -&gt; `...66`\n\n\n\ncountry_metadata &lt;- read_csv('worldbank/Metadata_Country_API_SP.POP.TOTL_DS2_en_csv_v2_3469297.csv', show_col_types = FALSE) %&gt;% \n  select(-`...6`) %&gt;%\n  rename(CountryCode = `Country Code`) %&gt;%\n  select(-SpecialNotes, -TableName)\n\nNew names:\n• `` -&gt; `...6`\n\n\n\npopdata &lt;- merge(population, country_metadata, by='CountryCode') %&gt;%\n  drop_na(Region)  # remove regional summaries\n\npopdata &lt;- merge(popdata, country_codes, by='CountryCode') %&gt;%\n  drop_na(Continent)\n\n\npopdata_with_geodata &lt;- merge(popdata, countries, by=\"CountryName\")\n\n\ngdp &lt;- read_csv('worldbank_gdp/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_3469429.csv', show_col_types = FALSE, skip=4) %&gt;% \n  select(-`...66`) %&gt;%\n  rename(CountryName = `Country Name`,\n         CountryCode = `Country Code`,\n         GDP2020 = `2020`)  %&gt;%\n  select(CountryCode, GDP2020) %&gt;%\n  drop_na(GDP2020)\n\nNew names:\n• `` -&gt; `...66`\n\npopdata_with_geodata_and_gdp &lt;- merge(popdata_with_geodata, gdp)\n\n\nwrite_csv(popdata_with_geodata_and_gdp, 'country_data.csv')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sondre Hølleland,  Assistant professor\n\n\n\n\n\nGeir Drage Berentsen,  Associate professor\n\n\n\nThis course and website has been developed by Sondre Hølleland, assistant professor, and Geir Drage Berentsen, associate professor, both at Norwegian School of Economics, Department of Business and Management Science. More about us will be added."
  },
  {
    "objectID": "01-videos.html",
    "href": "01-videos.html",
    "title": "Video lectures",
    "section": "",
    "text": "Video lectures",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Video lectures"
    ]
  },
  {
    "objectID": "01-exercises.html",
    "href": "01-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Exercises"
    ]
  },
  {
    "objectID": "01-python.html",
    "href": "01-python.html",
    "title": "Useful python commands",
    "section": "",
    "text": "Useful python commands",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Useful python commands"
    ]
  },
  {
    "objectID": "01-visualization.html",
    "href": "01-visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "On January 28, 1986, the Space Shuttle Challenger exploded 73 seconds after takeoff, killing all 7 of the astronauts on board. As when any such disaster occurs, there was an official investigation into the cause of the accident, which found that an O-ring connecting two sections of the solid rocket booster leaked, resulting in failure of the joint and explosion of the large liquid fuel tank (see figure @ref(fig:srbLeak)).\n\n\n\n\n\nAn image of the solid rocket booster leaking fuel, seconds before the explosion. The small flame visible on the side of the rocket is the site of the O-ring failure. By NASA (Great Images in NASA Description) [Public domain], via Wikimedia Commons\n\n\n\n\nThe investigation found that many aspects of the NASA decision making process were flawed, and focused in particular on a meeting between NASA staff and engineers from Morton Thiokol, a contractor who built the solid rocket boosters. These engineers were particularly concerned because the temperatures were forecast to be very cold on the morning of the launch, and they had data from previous launches showing that performance of the O-rings was compromised at lower temperatures. In a meeting on the evening before the launch, the engineers presented their data to the NASA managers, but were unable to convince them to postpone the launch. Their evidence was a set of hand-written slides showing numbers from various past launches.\nThe visualization expert Edward Tufte has argued that with a proper presentation of all of the data, the engineers could have been much more persuasive. In particular, they could have shown a figure like the one in Figure @ref(fig:challengerTemps), which highlights two important facts. First, it shows that the amount of O-ring damage (defined by the amount of erosion and soot found outside the rings after the solid rocket boosters were retrieved from the ocean in previous flights) was closely related to the temperature at takeoff. Second, it shows that the range of forecasted temperatures for the morning of January 28 (shown in the shaded area) was well outside of the range of all previous launches. While we can’t know for sure, it seems at least plausible that this could have been more persuasive.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA replotting of Tufte’s damage index data. The line shows the trend in the data, and the shaded patch shows the projected temperatures for the morning of the launch.\n\n\n\n\n\n\nThe goal of plotting data is to present a summary of a dataset in a two-dimensional (or occasionally three-dimensional) presentation. We refer to the dimensions as axes – the horizontal axis is called the X-axis and the vertical axis is called the Y-axis. We can arrange the data along the axes in a way that highlights the data values. These values may be either continuous or categorical.\nThere are many different types of plots that we can use, which have different advantages and disadvantages. Let’s say that we are interested in characterizing the difference in height between men and women in the NHANES dataset. Figure @ref(fig:plotHeight) shows four different ways to plot these data.\n\nThe bar graph in panel A shows the difference in means, but doesn’t show us how much spread there is in the data around these means – and as we will see later, knowing this is essential to determine whether we think the difference between the groups is large enough to be important.\n\nThe second plot shows the bars with all of the data points overlaid - this makes it a bit clearer that the distributions of height for men and women are overlapping, but it’s still hard to see due to the large number of data points.\n\nIn general we prefer using a plotting technique that provides a clearer view of the distribution of the data points.\n\nIn panel C, we see one example of a violin plot, which plots the distribution of data in each condition (after smoothing it out a bit).\n\nAnother option is the box plot shown in panel D, which shows the median (central line), a measure of variability (the width of the box, which is based on a measure called the interquartile range), and any outliers (noted by the points at the ends of the lines). These are both effective ways to show data that provide a good feel for the distribution of the data.\n\n\n\n\n\n\nFour different ways of plotting the difference in height between men and women in the NHANES dataset. Panel A plots the means of the two groups, which gives no way to assess the relative overlap of the two distributions. Panel B shows the same bars, but also overlays the data points, jittering them so that we can see their overall distribution. Panel C shows a violin plot, which shows the distribution of the datasets for each group. Panel D shows a box plot, which highlights the spread of the distribution along with any outliers (which are shown as individual points).\n\n\n\n\n\n\n\nMany books have been written on effective visualization of data. There are some principles that most of these authors agree on, while others are more contentious. Here we summarize some of the major principles; if you want to learn more, then some good resources are listed in the Suggested Readings section at the end of this chapter.\n\n\nLet’s say that I performed a study that examined the relationship between dental health and time spent flossing, and I would like to visualize my data. Figure @ref(fig:dentalFigs) shows four possible presentations of these data.\n\nIn panel A, we don’t actually show the data, just a line expressing the relationship between the data. This is clearly not optimal, because we can’t actually see what the underlying data look like.\n\nPanels B-D show three possible outcomes from plotting the actual data, where each plot shows a different way that the data might have looked.\n\nIf we saw the plot in Panel B, we would probably be suspicious – rarely would real data follow such a precise pattern.\n\nThe data in Panel C, on the other hand, look like real data – they show a general trend, but they are messy, as data in the world usually are.\n\nThe data in Panel D show us that the apparent relationship between the two variables is solely caused by one individual, who we would refer to as an outlier because they fall so far outside of the pattern of the rest of the group. It should be clear that we probably don’t want to conclude very much from an effect that is driven by one data point. This figure highlights why it is always important to look at the raw data before putting too much faith in any summary of the data.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFour different possible presentations of data for the dental health example. Each point in the scatter plot represents one data point in the dataset, and the line in each plot represents the linear trend in the data.\n\n\n\n\n\n\n\nEdward Tufte has proposed an idea called the data/ink ratio:\n\\[\ndata/ink\\ ratio = \\frac{amount\\, of\\, ink\\, used\\, on\\, data}{total\\, amount\\, of\\, ink}\n\\] The point of this is to minimize visual clutter and let the data show through. For example, take the two presentations of the dental health data in Figure @ref(fig:dataInkExample). Both panels show the same data, but panel A is much easier to apprehend, because of its relatively higher data/ink ratio.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_smooth()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAn example of the same data plotted with two different data/ink ratios.\n\n\n\n\n\n\n\nIt’s especially common to see presentations of data in the popular media that are adorned with lots of visual elements that are thematically related to the content but unrelated to the actual data. This is known as chartjunk, and should be avoided at all costs.\nOne good way to avoid chartjunk is to avoid using popular spreadsheet programs to plot one’s data. For example, the chart in Figure @ref(fig:chartJunk) (created using Microsoft Excel) plots the relative popularity of different religions in the United States. There are at least three things wrong with this figure:\n\nit has graphics overlaid on each of the bars that have nothing to do with the actual data\nit has a distracting background texture\nit uses three-dimensional bars, which distort the data\n\n\n\n\n\n\nAn example of chart junk.\n\n\n\n\n\n\n\nIt’s often possible to use visualization to distort the message of a dataset. A very common one is use of different axis scaling to either exaggerate or hide a pattern of data. For example, let’s say that we are interested in seeing whether rates of violent crime have changed in the US. In Figure @ref(fig:crimePlotAxes), we can see these data plotted in ways that either make it look like crime has remained constant, or that it has plummeted. The same data can tell two very different stories!\n\n\n\n\n\nCrime data from 1990 to 2014 plotted over time. Panels A and B show the same data, but with different ranges of values along the Y axis. Data obtained from https://www.ucrdatatool.gov/Search/Crime/State/RunCrimeStatebyState.cfm\n\n\n\n\nOne of the major controversies in statistical data visualization is how to choose the Y axis, and in particular whether it should always include zero. In his famous book “How to lie with statistics”, Darrell Huff argued strongly that one should always include the zero point in the Y axis. On the other hand, Edward Tufte has argued against this:\n\n“In general, in a time-series, use a baseline that shows the data not the zero point; don’t spend a lot of empty vertical space trying to reach down to the zero point at the cost of hiding what is going on in the data line itself.” (from https://qz.com/418083/its-ok-not-to-start-your-y-axis-at-zero/)\n\nThere are certainly cases where using the zero point makes no sense at all. Let’s say that we are interested in plotting body temperature for an individual over time. In Figure @ref(fig:bodyTempAxis) we plot the same (simulated) data with or without zero in the Y axis. It should be obvious that by plotting these data with zero in the Y axis (Panel A) we are wasting a lot of space in the figure, given that body temperature of a living person could never go to zero! By including zero, we are also making the apparent jump in temperature during days 21-30 much less evident. In general, my inclination for line plots and scatterplots is to use all of the space in the graph, unless the zero point is truly important to highlight.\n\n\n\n\n\nBody temperature over time, plotted with or without the zero point in the Y axis.\n\n\n\n\nEdward Tufte introduced the concept of the lie factor to describe the degree to which physical differences in a visualization correspond to the magnitude of the differences in the data. If a graphic has a lie factor near 1, then it is appropriately representing the data, whereas lie factors far from one reflect a distortion of the underlying data.\nThe lie factor supports the argument that one should always include the zero point in a bar chart in many cases. In Figure @ref(fig:barCharLieFactor) we plot the same data with and without zero in the Y axis. In panel A, the proportional difference in area between the two bars is exactly the same as the proportional difference between the values (i.e. lie factor = 1), whereas in Panel B (where zero is not included) the proportional difference in area between the two bars is roughly 2.8 times bigger than the proportional difference in the values, and thus it visually exaggerates the size of the difference.\n\n\n\n\n\nTwo bar charts with associated lie factors.\n\n\n\n\n\n\n\n\nHumans have both perceptual and cognitive limitations that can make some visualizations very difficult to understand. It’s always important to keep these in mind when building a visualization.\n\n\nOne important perceptual limitation that many people (including myself) suffer from is color blindness. This can make it very difficult to perceive the information in a figure (like the one in Figure @ref(fig:badColors)) where there is only color contrast between the elements but no brightness contrast. It is always helpful to use graph elements that differ substantially in brightness and/or texture, in addition to color. There are also “colorblind-friendly” palettes available for use with many visualization tools.\n\n\n\n\n\nExample of a bad figure that relies solely on color contrast.\n\n\n\n\nEven for people with perfect color vision, there are perceptual limitations that can make some plots ineffective. This is one reason why statisticians never use pie charts: It can be very difficult for humans to accurately perceive differences in the volume of shapes. The pie chart in Figure @ref(fig:pieChart) (presenting the same data on religious affiliation that we showed above) shows how tricky this can be.\n\n\n\n\n\nAn example of a pie chart, highlighting the difficulty in apprehending the relative volume of the different pie slices.\n\n\n\n\nThis plot is terrible for several reasons. First, it requires distinguishing a large number of colors from very small patches at the bottom of the figure. Second, the visual perspective distorts the relative numbers, such that the pie wedge for Catholic appears much larger than the pie wedge for None, when in fact the number for None is slightly larger (22.8 vs 20.8 percent), as was evident in Figure @ref(fig:chartJunk). Third, by separating the legend from the graphic, it requires the viewer to hold information in their working memory in order to map between the graphic and legend and to conduct many “table look-ups” in order to continuously match the legend labels to the visualization. And finally, it uses text that is far too small, making it impossible to read without zooming in.\nPlotting the data using a more reasonable approach (Figure @ref(fig:religionBars)), we can see the pattern much more clearly. This plot may not look as flashy as the pie chart generated using Excel, but it’s a much more effective and accurate representation of the data.\n\n\n\n\n\nA clearer presentation of the religious affiliation data (obtained from http://www.pewforum.org/religious-landscape-study/).\n\n\n\n\nThis plot allows the viewer to make comparisons based on the the length of the bars along a common scale (the y-axis). Humans tend to be more accurate when decoding differences based on these perceptual elements than based on area or color.\n\n\n\n\nOften we are interested in plotting data where the variable of interest is affected by other factors than the one we are interested in. For example, let’s say that we want to understand how the price of gasoline has changed over time. Figure @ref(fig:gasPrices) shows historical gas price data, plotted either with or without adjustment for inflation. Whereas the unadjusted data show a huge increase, the adjusted data show that this is mostly just reflective of inflation. Other examples where one needs to adjust data for other factors include population size and data collected across different seasons.\n\n\n\n\n\nThe price of gasoline in the US from 1930 to 2013 (obtained from http://www.thepeoplehistory.com/70yearsofpricechange.html) with or without correction for inflation (based on Consumer Price Index).\n\n\n\n\n\n\n\nHaving read this chapter, you should be able to:\n\nDescribe the principles that distinguish between good and bad graphs, and use them to identify good versus bad graphs.\nUnderstand the human limitations that must be accommodated in order to make effective graphs.\nPromise to never create a pie chart. Ever.\n\n\n\n\n\nFundamentals of Data Visualization, by Claus Wilke\nVisual Explanations, by Edward Tufte\nVisualizing Data, by William S. Cleveland\nGraph Design for the Eye and Mind, by Stephen M. Kosslyn\nHow Humans See Data, by John Rauser"
  },
  {
    "objectID": "01-visualization.html#anatomy-of-a-plot",
    "href": "01-visualization.html#anatomy-of-a-plot",
    "title": "Data Visualization",
    "section": "",
    "text": "The goal of plotting data is to present a summary of a dataset in a two-dimensional (or occasionally three-dimensional) presentation. We refer to the dimensions as axes – the horizontal axis is called the X-axis and the vertical axis is called the Y-axis. We can arrange the data along the axes in a way that highlights the data values. These values may be either continuous or categorical.\nThere are many different types of plots that we can use, which have different advantages and disadvantages. Let’s say that we are interested in characterizing the difference in height between men and women in the NHANES dataset. Figure @ref(fig:plotHeight) shows four different ways to plot these data.\n\nThe bar graph in panel A shows the difference in means, but doesn’t show us how much spread there is in the data around these means – and as we will see later, knowing this is essential to determine whether we think the difference between the groups is large enough to be important.\n\nThe second plot shows the bars with all of the data points overlaid - this makes it a bit clearer that the distributions of height for men and women are overlapping, but it’s still hard to see due to the large number of data points.\n\nIn general we prefer using a plotting technique that provides a clearer view of the distribution of the data points.\n\nIn panel C, we see one example of a violin plot, which plots the distribution of data in each condition (after smoothing it out a bit).\n\nAnother option is the box plot shown in panel D, which shows the median (central line), a measure of variability (the width of the box, which is based on a measure called the interquartile range), and any outliers (noted by the points at the ends of the lines). These are both effective ways to show data that provide a good feel for the distribution of the data.\n\n\n\n\n\n\nFour different ways of plotting the difference in height between men and women in the NHANES dataset. Panel A plots the means of the two groups, which gives no way to assess the relative overlap of the two distributions. Panel B shows the same bars, but also overlays the data points, jittering them so that we can see their overall distribution. Panel C shows a violin plot, which shows the distribution of the datasets for each group. Panel D shows a box plot, which highlights the spread of the distribution along with any outliers (which are shown as individual points)."
  },
  {
    "objectID": "01-visualization.html#principles-of-good-visualization",
    "href": "01-visualization.html#principles-of-good-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "Many books have been written on effective visualization of data. There are some principles that most of these authors agree on, while others are more contentious. Here we summarize some of the major principles; if you want to learn more, then some good resources are listed in the Suggested Readings section at the end of this chapter.\n\n\nLet’s say that I performed a study that examined the relationship between dental health and time spent flossing, and I would like to visualize my data. Figure @ref(fig:dentalFigs) shows four possible presentations of these data.\n\nIn panel A, we don’t actually show the data, just a line expressing the relationship between the data. This is clearly not optimal, because we can’t actually see what the underlying data look like.\n\nPanels B-D show three possible outcomes from plotting the actual data, where each plot shows a different way that the data might have looked.\n\nIf we saw the plot in Panel B, we would probably be suspicious – rarely would real data follow such a precise pattern.\n\nThe data in Panel C, on the other hand, look like real data – they show a general trend, but they are messy, as data in the world usually are.\n\nThe data in Panel D show us that the apparent relationship between the two variables is solely caused by one individual, who we would refer to as an outlier because they fall so far outside of the pattern of the rest of the group. It should be clear that we probably don’t want to conclude very much from an effect that is driven by one data point. This figure highlights why it is always important to look at the raw data before putting too much faith in any summary of the data.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFour different possible presentations of data for the dental health example. Each point in the scatter plot represents one data point in the dataset, and the line in each plot represents the linear trend in the data.\n\n\n\n\n\n\n\nEdward Tufte has proposed an idea called the data/ink ratio:\n\\[\ndata/ink\\ ratio = \\frac{amount\\, of\\, ink\\, used\\, on\\, data}{total\\, amount\\, of\\, ink}\n\\] The point of this is to minimize visual clutter and let the data show through. For example, take the two presentations of the dental health data in Figure @ref(fig:dataInkExample). Both panels show the same data, but panel A is much easier to apprehend, because of its relatively higher data/ink ratio.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_smooth()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAn example of the same data plotted with two different data/ink ratios.\n\n\n\n\n\n\n\nIt’s especially common to see presentations of data in the popular media that are adorned with lots of visual elements that are thematically related to the content but unrelated to the actual data. This is known as chartjunk, and should be avoided at all costs.\nOne good way to avoid chartjunk is to avoid using popular spreadsheet programs to plot one’s data. For example, the chart in Figure @ref(fig:chartJunk) (created using Microsoft Excel) plots the relative popularity of different religions in the United States. There are at least three things wrong with this figure:\n\nit has graphics overlaid on each of the bars that have nothing to do with the actual data\nit has a distracting background texture\nit uses three-dimensional bars, which distort the data\n\n\n\n\n\n\nAn example of chart junk.\n\n\n\n\n\n\n\nIt’s often possible to use visualization to distort the message of a dataset. A very common one is use of different axis scaling to either exaggerate or hide a pattern of data. For example, let’s say that we are interested in seeing whether rates of violent crime have changed in the US. In Figure @ref(fig:crimePlotAxes), we can see these data plotted in ways that either make it look like crime has remained constant, or that it has plummeted. The same data can tell two very different stories!\n\n\n\n\n\nCrime data from 1990 to 2014 plotted over time. Panels A and B show the same data, but with different ranges of values along the Y axis. Data obtained from https://www.ucrdatatool.gov/Search/Crime/State/RunCrimeStatebyState.cfm\n\n\n\n\nOne of the major controversies in statistical data visualization is how to choose the Y axis, and in particular whether it should always include zero. In his famous book “How to lie with statistics”, Darrell Huff argued strongly that one should always include the zero point in the Y axis. On the other hand, Edward Tufte has argued against this:\n\n“In general, in a time-series, use a baseline that shows the data not the zero point; don’t spend a lot of empty vertical space trying to reach down to the zero point at the cost of hiding what is going on in the data line itself.” (from https://qz.com/418083/its-ok-not-to-start-your-y-axis-at-zero/)\n\nThere are certainly cases where using the zero point makes no sense at all. Let’s say that we are interested in plotting body temperature for an individual over time. In Figure @ref(fig:bodyTempAxis) we plot the same (simulated) data with or without zero in the Y axis. It should be obvious that by plotting these data with zero in the Y axis (Panel A) we are wasting a lot of space in the figure, given that body temperature of a living person could never go to zero! By including zero, we are also making the apparent jump in temperature during days 21-30 much less evident. In general, my inclination for line plots and scatterplots is to use all of the space in the graph, unless the zero point is truly important to highlight.\n\n\n\n\n\nBody temperature over time, plotted with or without the zero point in the Y axis.\n\n\n\n\nEdward Tufte introduced the concept of the lie factor to describe the degree to which physical differences in a visualization correspond to the magnitude of the differences in the data. If a graphic has a lie factor near 1, then it is appropriately representing the data, whereas lie factors far from one reflect a distortion of the underlying data.\nThe lie factor supports the argument that one should always include the zero point in a bar chart in many cases. In Figure @ref(fig:barCharLieFactor) we plot the same data with and without zero in the Y axis. In panel A, the proportional difference in area between the two bars is exactly the same as the proportional difference between the values (i.e. lie factor = 1), whereas in Panel B (where zero is not included) the proportional difference in area between the two bars is roughly 2.8 times bigger than the proportional difference in the values, and thus it visually exaggerates the size of the difference.\n\n\n\n\n\nTwo bar charts with associated lie factors."
  },
  {
    "objectID": "01-visualization.html#accommodating-human-limitations",
    "href": "01-visualization.html#accommodating-human-limitations",
    "title": "Data Visualization",
    "section": "",
    "text": "Humans have both perceptual and cognitive limitations that can make some visualizations very difficult to understand. It’s always important to keep these in mind when building a visualization.\n\n\nOne important perceptual limitation that many people (including myself) suffer from is color blindness. This can make it very difficult to perceive the information in a figure (like the one in Figure @ref(fig:badColors)) where there is only color contrast between the elements but no brightness contrast. It is always helpful to use graph elements that differ substantially in brightness and/or texture, in addition to color. There are also “colorblind-friendly” palettes available for use with many visualization tools.\n\n\n\n\n\nExample of a bad figure that relies solely on color contrast.\n\n\n\n\nEven for people with perfect color vision, there are perceptual limitations that can make some plots ineffective. This is one reason why statisticians never use pie charts: It can be very difficult for humans to accurately perceive differences in the volume of shapes. The pie chart in Figure @ref(fig:pieChart) (presenting the same data on religious affiliation that we showed above) shows how tricky this can be.\n\n\n\n\n\nAn example of a pie chart, highlighting the difficulty in apprehending the relative volume of the different pie slices.\n\n\n\n\nThis plot is terrible for several reasons. First, it requires distinguishing a large number of colors from very small patches at the bottom of the figure. Second, the visual perspective distorts the relative numbers, such that the pie wedge for Catholic appears much larger than the pie wedge for None, when in fact the number for None is slightly larger (22.8 vs 20.8 percent), as was evident in Figure @ref(fig:chartJunk). Third, by separating the legend from the graphic, it requires the viewer to hold information in their working memory in order to map between the graphic and legend and to conduct many “table look-ups” in order to continuously match the legend labels to the visualization. And finally, it uses text that is far too small, making it impossible to read without zooming in.\nPlotting the data using a more reasonable approach (Figure @ref(fig:religionBars)), we can see the pattern much more clearly. This plot may not look as flashy as the pie chart generated using Excel, but it’s a much more effective and accurate representation of the data.\n\n\n\n\n\nA clearer presentation of the religious affiliation data (obtained from http://www.pewforum.org/religious-landscape-study/).\n\n\n\n\nThis plot allows the viewer to make comparisons based on the the length of the bars along a common scale (the y-axis). Humans tend to be more accurate when decoding differences based on these perceptual elements than based on area or color."
  },
  {
    "objectID": "01-visualization.html#correcting-for-other-factors",
    "href": "01-visualization.html#correcting-for-other-factors",
    "title": "Data Visualization",
    "section": "",
    "text": "Often we are interested in plotting data where the variable of interest is affected by other factors than the one we are interested in. For example, let’s say that we want to understand how the price of gasoline has changed over time. Figure @ref(fig:gasPrices) shows historical gas price data, plotted either with or without adjustment for inflation. Whereas the unadjusted data show a huge increase, the adjusted data show that this is mostly just reflective of inflation. Other examples where one needs to adjust data for other factors include population size and data collected across different seasons.\n\n\n\n\n\nThe price of gasoline in the US from 1930 to 2013 (obtained from http://www.thepeoplehistory.com/70yearsofpricechange.html) with or without correction for inflation (based on Consumer Price Index)."
  },
  {
    "objectID": "01-visualization.html#learning-objectives",
    "href": "01-visualization.html#learning-objectives",
    "title": "Data Visualization",
    "section": "",
    "text": "Having read this chapter, you should be able to:\n\nDescribe the principles that distinguish between good and bad graphs, and use them to identify good versus bad graphs.\nUnderstand the human limitations that must be accommodated in order to make effective graphs.\nPromise to never create a pie chart. Ever."
  },
  {
    "objectID": "01-visualization.html#suggested-readings-and-videos",
    "href": "01-visualization.html#suggested-readings-and-videos",
    "title": "Data Visualization",
    "section": "",
    "text": "Fundamentals of Data Visualization, by Claus Wilke\nVisual Explanations, by Edward Tufte\nVisualizing Data, by William S. Cleveland\nGraph Design for the Eye and Mind, by Stephen M. Kosslyn\nHow Humans See Data, by John Rauser"
  },
  {
    "objectID": "2_2_random_variables.html",
    "href": "2_2_random_variables.html",
    "title": "Random Variables",
    "section": "",
    "text": "Random Variables",
    "crumbs": [
      "Modules",
      "Module 2: Probability",
      "Random Variables"
    ]
  },
  {
    "objectID": "calender.html",
    "href": "calender.html",
    "title": "Calender",
    "section": "",
    "text": "Calender"
  },
  {
    "objectID": "datalab-1.html",
    "href": "datalab-1.html",
    "title": "Datalab 1",
    "section": "",
    "text": "Datalab 1",
    "crumbs": [
      "Datalabs",
      "Datalab 1"
    ]
  },
  {
    "objectID": "datalab-3.html",
    "href": "datalab-3.html",
    "title": "Datalab 3",
    "section": "",
    "text": "Datalab 3"
  },
  {
    "objectID": "module-1.html",
    "href": "module-1.html",
    "title": "Summarizing data",
    "section": "",
    "text": "I mentioned in the Introduction that one of the big discoveries of statistics is the idea that we can better understand the world by throwing away information, and that’s exactly what we are doing when we summarize a dataset. In this Chapter we will discuss why and how to summarize data.\n\n\nWhen we summarize data, we are necessarily throwing away information, and one might plausibly object to this. As an example, let’s go back to the PURE study that we discussed in Chapter 1. Are we not supposed to believe that all of the details about each individual matter, beyond those that are summarized in the dataset? What about the specific details of how the data were collected, such as the time of day or the mood of the participant? All of these details are lost when we summarize the data.\nOne reason that we summarize data is that it provides us with a way to generalize - that is, to make general statements that extend beyond specific observations. The importance of generalization was highlighted by the writer Jorge Luis Borges in his short story “Funes the Memorious”, which describes an individual who loses the ability to forget. Borges focuses in on the relation between generalization (i.e. throwing away data) and thinking: “To think is to forget a difference, to generalize, to abstract. In the overly replete world of Funes, there were nothing but details.”\nPsychologists have long studied all of the ways in which generalization is central to thinking. One example is categorization: We are able to easily recognize different examples of the category of “birds” even though the individual examples may be very different in their surface features (such as an ostrich, a robin, and a chicken). Importantly, generalization lets us make predictions about these individuals – in the case of birds, we can predict that they can fly and eat seeds, and that they probably can’t drive a car or speak English. These predictions won’t always be right, but they are often good enough to be useful in the world.\n\n\n\nA simple way to summarize data is to generate a table representing counts of various types of observations. This type of table has been used for thousands of years (see Figure @ref(fig:salesContract)).\n\n\n\n\n\nA Sumerian tablet from the Louvre, showing a sales contract for a house and field. Public domain, via Wikimedia Commons.\n\n\n\n\nLet’s look at some examples of the use of tables, using a more realistic dataset. Throughout this book we will use the National Health and Nutrition Examination Survey (NHANES) dataset. This is an ongoing study that assesses the health and nutrition status of a sample of individuals from the United States on many different variables. We will use a version of the dataset that is available for the R statistical software package. For this example, we will look at a simple variable, called PhysActive in the dataset. This variable contains one of three different values: “Yes” or “No” (indicating whether or not the person reports doing “moderate or vigorous-intensity sports, fitness or recreational activities”), or “NA” if the data are missing for that individual. There are different reasons that the data might be missing; for example, this question was not asked of children younger than 12 years of age, while in other cases an adult may have declined to answer the question during the interview, or the interviewer’s recording of the answer on their form might be unreadable.\n\n\nA distribution describes how data are divided between different possible values. For this example, let’s look at how many people fall into each of the physical activity categories.\n\n\n\nFrequency distribution for PhysActive variable\n\n\nPhysActive\nAbsoluteFrequency\n\n\n\n\nNo\n2473\n\n\nYes\n2972\n\n\nNA\n1334\n\n\n\n\n\nTable @ref(tab:PhysActiveTable) shows the frequencies of each of the different values; there were 2473 individuals who responded “No” to the question, 2972 who responded “Yes”, and 1334 for whom no response was given. We call this a frequency distribution because it tells us how frequent each of the possible values is within our sample.\nThis shows us the absolute frequency of the two responses, for everyone who actually gave a response. We can see from this that there are more people saying “Yes” than “No”, but it can be hard to tell from absolute numbers how big the difference is in relative terms. For this reason, we often would rather present the data using relative frequency, which is obtained by dividing each frequency by the sum of all frequencies:\n\\[\nrelative\\ frequency_i = \\frac{absolute\\ frequency_i}{\\sum_{j=1}^N absolute\\ frequency_j}\n\\] The relative frequency provides a much easier way to see how big the imbalance is. We can also interpret the relative frequencies as percentages by multiplying them by 100. In this example, we will drop the NA values as well, since we would like to be able to interpret the relative frequencies of active versus inactive people. However, for this to make sense we have to assume that the NA values are missing “at random”, meaning that their presence or absence is not related to the true value of the variable for that person. For example, if inactive participants were more likely to refuse to answer the question than active participants, then that would bias our estimate of the frequency of physical activity, meaning that our estimate would be different from the true value.\n\n\n\nAbsolute and relative frequencies and percentages for PhysActive variable\n\n\nPhysActive\nAbsoluteFrequency\nRelativeFrequency\nPercentage\n\n\n\n\nNo\n2473\n0.45\n45\n\n\nYes\n2972\n0.55\n55\n\n\n\n\n\nTable @ref(tab:PhysActiveTableFiltered) lets us see that 45.4 percent of the individuals in the NHANES sample said “No” and 54.6 percent said “Yes”.\n\n\n\nThe PhysActive variable that we examined above only had two possible values, but often we wish to summarize data that can have many more possible values. When those values are quantitative, then one useful way to summarize them is via what we call a cumulative frequency representation: rather than asking how many observations take on a specific value, we ask how many have a value some specific value or less.\nLet’s look at another variable in the NHANES dataset, called SleepHrsNight which records how many hours the participant reports sleeping on usual weekdays. Table @ref(tab:sleepTable) shows a frequency table created as we did above, after removing anyone with missing data for this question. We can already begin to summarize the dataset just by looking at the table; for example, we can see that most people report sleeping between 6 and 8 hours. To see this even more clearly, we can plot a histogram which shows the number of cases having each of the different values; see left panel of Figure @ref(fig:sleepHist). We can also plot the relative frequencies, which we will often refer to as densities - see the right panel of Figure @ref(fig:sleepHist).\n\n\n\nFrequency distribution for number of hours of sleep per night in the NHANES dataset\n\n\nSleepHrsNight\nAbsoluteFrequency\nRelativeFrequency\nPercentage\n\n\n\n\n2\n9\n0.00\n0.18\n\n\n3\n49\n0.01\n0.97\n\n\n4\n200\n0.04\n3.97\n\n\n5\n406\n0.08\n8.06\n\n\n6\n1172\n0.23\n23.28\n\n\n7\n1394\n0.28\n27.69\n\n\n8\n1405\n0.28\n27.90\n\n\n9\n271\n0.05\n5.38\n\n\n10\n97\n0.02\n1.93\n\n\n11\n15\n0.00\n0.30\n\n\n12\n17\n0.00\n0.34\n\n\n\n\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nLeft: Histogram showing the number (left) and proportion (right) of people reporting each possible value of the SleepHrsNight variable.\n\n\n\n\nWhat if we want to know how many people report sleeping 5 hours or less? To find this, we can compute a cumulative distribution. To compute the cumulative frequency for some value j, we add up the frequencies for all of the values up to and including j:\n\\[\ncumulative\\ frequency_j = \\sum_{i=1}^{j}{absolute\\ frequency_i}\n\\]\n\n\n\n\nAbsolute and cumulative frequency distributions for SleepHrsNight variable\n\n\nSleepHrsNight\nAbsoluteFrequency\nCumulativeFrequency\n\n\n\n\n2\n9\n9\n\n\n3\n49\n58\n\n\n4\n200\n258\n\n\n5\n406\n664\n\n\n6\n1172\n1836\n\n\n7\n1394\n3230\n\n\n8\n1405\n4635\n\n\n9\n271\n4906\n\n\n10\n97\n5003\n\n\n11\n15\n5018\n\n\n12\n17\n5035\n\n\n\n\n\nLet’s do this for our sleep variable, computing the absolute and cumulative frequency. In the left panel of Figure @ref(fig:sleepAbsCumulRelFreq) we plot the data to see what these representations look like; the absolute frequency values are plotted in solid lines, and the cumulative frequencies are plotted in dashed lines We see that the cumulative frequency is monotonically increasing – that is, it can only go up or stay constant, but it can never decrease. Again, we usually find the relative frequencies to be more useful than the absolute; those are plotted in the right panel of Figure @ref(fig:sleepAbsCumulRelFreq). Importantly, the shape of the relative frequency plot is exactly the same as the absolute frequency plot – only the size of the values has changed.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nA plot of the relative (solid) and cumulative relative (dashed) values for frequency (left) and proportion (right) for the possible values of SleepHrsNight.\n\n\n\n\n\n\n\n\n\n\n\n\nA histogram of the Age (left) and Height (right) variables in NHANES.\n\n\n\n\nThe variables that we examined above were fairly simple, having only a few possible values. Now let’s look at a more complex variable: Age. First let’s plot the Age variable for all of the individuals in the NHANES dataset (see left panel of Figure @ref(fig:ageHist)). What do you see there? First, you should notice that the number of individuals in each age group is declining over time. This makes sense because the population is being randomly sampled, and thus death over time leads to fewer people in the older age ranges. Second, you probably notice a large spike in the graph at age 80. What do you think that’s about?\nIf were were to look up the information about the NHANES dataset, we would see the following definition for the Age variable: “Age in years at screening of study participant. Note: Subjects 80 years or older were recorded as 80.” The reason for this is that the relatively small number of individuals with very high ages would make it potentially easier to identify the specific person in the dataset if you knew their exact age; researchers generally promise their participants to keep their identity confidential, and this is one of the things they can do to help protect their research subjects. This also highlights the fact that it’s always important to know where one’s data have come from and how they have been processed; otherwise we might interpret them improperly, thinking that 80-year-olds had been somehow overrepresented in the sample.\nLet’s look at another more complex variable in the NHANES dataset: Height. The histogram of height values is plotted in the right panel of Figure @ref(fig:ageHist). The first thing you should notice about this distribution is that most of its density is centered around about 170 cm, but the distribution has a “tail” on the left; there are a small number of individuals with much smaller heights. What do you think is going on here?\nYou may have intuited that the small heights are coming from the children in the dataset. One way to examine this is to plot the histogram with separate colors for children and adults (left panel of Figure @ref(fig:heightHistSep)). This shows that all of the very short heights were indeed coming from children in the sample. Let’s create a new version of NHANES that only includes adults, and then plot the histogram just for them (right panel of Figure @ref(fig:heightHistSep)). In that plot the distribution looks much more symmetric. As we will see later, this is a nice example of a normal (or Gaussian) distribution.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\nHistogram of heights for NHANES. A: values plotted separately for children (gray) and adults (black). B: values for adults only. C: Same as B, but with bin width = 0.1\n\n\n\n\n\n\n\nIn our earlier example with the sleep variable, the data were reported in whole numbers, and we simply counted the number of people who reported each possible value. However, if you look at a few values of the Height variable in NHANES (as shown in Table @ref(tab:heightVals)), you will see that it was measured in centimeters down to the first decimal place.\n\n\n\nA few values of Height from the NHANES data frame.\n\n\nHeight\n\n\n\n\n169.6\n\n\n169.8\n\n\n167.5\n\n\n155.2\n\n\n173.8\n\n\n174.5\n\n\n\n\n\nPanel C of Figure @ref(fig:heightHistSep) shows a histogram that counts the density of each possible value down the first decimal place. That histogram looks really jagged, which is because of the variability in specific decimal place values. For example, the value 173.2 occurs 32 times, while the value 173.3 only occurs 15 times. We probably don’t think that there is really such a big difference between the prevalence of these two heights; more likely this is just due to random variability in our sample of people.\nIn general, when we create a histogram of data that are continuous or where there are many possible values, we will bin the values so that instead of counting and plotting the frequency of every specific value, we count and plot the frequency of values falling within specific ranges. That’s why the plot looked less jagged above in Panel B of @ref(fig:heightHistSep); in this panel we set the bin width to 1, which means that the histogram is computed by combining values within bins with a width of one; thus, the values 1.3, 1.5, and 1.6 would all count toward the frequency of the same bin, which would span from values equal to one up through values less than 2.\nNote that once the bin size has been selected, then the number of bins is determined by the data:\n\\[\nnumber\\, of\\, bins  = \\frac{range\\, of\\, scores}{bin\\, width}\n\\]\nThere is no hard and fast rule for how to choose the optimal bin width. Occasionally it will be obvious (as when there are only a few possible values), but in many cases it would require trial and error. There are methods that try to find an optimal bin size automatically, such as the Freedman-Diaconis method that we will use in some later examples.\n\n\n\n\nDatasets are like snowflakes, in that every one is different, but nonetheless there are patterns that one often sees in different types of data. This allows us to use idealized representations of the data to further summarize them. Let’s take the adult height data plotted in @ref(fig:heightHistSep), and plot them alongside a very different variable: pulse rate (heartbeats per minute), also measured in NHANES (see Figure @ref(fig:NormalDistPlotsWithDist)).\n\n\n\n\n\nHistograms for height (left) and pulse (right) in the NHANES dataset, with the normal distribution overlaid for each dataset.\n\n\n\n\nWhile these plots certainly don’t look exactly the same, both have the general characteristic of being relatively symmetric around a rounded peak in the middle. This shape is in fact one of the commonly observed shapes of distributions when we collect data, which we call the normal (or Gaussian) distribution. This distribution is defined in terms of two values (which we call parameters of the distribution): the location of the center peak (which we call the mean) and the width of the distribution (which is described in terms of a parameter called the standard deviation). Figure @ref(fig:NormalDistPlotsWithDist) shows the appropriate normal distribution plotted on top of each of the histrograms.You can see that although the curves don’t fit the data exactly, they do a pretty good job of characterizing the distribution – with just two numbers!\nAs we will see later when we discuss the central limit theorem, there is a deep mathematical reason why many variables in the world exhibit the form of a normal distribution.\n\n\nThe examples in Figure @ref(fig:NormalDistPlotsWithDist) followed the normal distribution fairly well, but in many cases the data will deviate in a systematic way from the normal distribution. One way in which the data can deviate is when they are asymmetric, such that one tail of the distribution is more dense than the other. We refer to this as “skewness”. Skewness commonly occurs when the measurement is constrained to be non-negative, such as when we are counting things or measuring elapsed times (and thus the variable can’t take on negative values).\nAn example of relatively mild skewness can be seen in the average waiting times at the airport security lines at San Francisco International Airport, plotted in the left panel of Figure @ref(fig:SFOWaitTimes). You can see that while most wait times are less than 20 minutes, there are a number of cases where they are much longer, over 60 minutes! This is an example of a “right-skewed” distribution, where the right tail is longer than the left; these are common when looking at counts or measured times, which can’t be less than zero. It’s less common to see “left-skewed” distributions, but they can occur, for example when looking at fractional values that can’t take a value greater than one.\n\n\n\n\n\nExamples of right-skewed and long-tailed distributions. Left: Average wait times for security at SFO Terminal A (Jan-Oct 2017), obtained from https://awt.cbp.gov/ . Right: A histogram of the number of Facebook friends amongst 3,663 individuals, obtained from the Stanford Large Network Database. The person with the maximum number of friends is indicated by the diamond.\n\n\n\n\n\n\n\nHistorically, statistics has focused heavily on data that are normally distributed, but there are many data types that look nothing like the normal distribution. In particular, many real-world distributions are “long-tailed”, meaning that the right tail extends far beyond the most typical members of the distribution; that is, they are extremely skewed. One of the most interesting types of data where long-tailed distributions occur arises from the analysis of social networks. For an example, let’s look at the Facebook friend data from the Stanford Large Network Database and plot the histogram of number of friends across the 3,663 people in the database (see right panel of Figure @ref(fig:SFOWaitTimes)). As we can see, this distribution has a very long right tail – the average person has 24.09 friends, while the person with the most friends (denoted by the blue dot) has 1043!\nLong-tailed distributions are increasingly being recognized in the real world. In particular, many features of complex systems are characterized by these distributions, from the frequency of words in text, to the number of flights in and out of different airports, to the connectivity of brain networks. There are a number of different ways that long-tailed distributions can come about, but a common one occurs in cases of the so-called “Matthew effect” from the Christian Bible:\n\nFor to every one who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away. — Matthew 25:29, Revised Standard Version\n\nThis is often paraphrased as “the rich get richer”. In these situations, advantages compound, such that those with more friends have access to even more new friends, and those with more money have the ability to do things that increase their riches even more.\nAs the course progresses we will see several examples of long-tailed distributions, and we should keep in mind that many of the tools in statistics can fail when faced with long-tailed data. As Nassim Nicholas Taleb pointed out in his book “The Black Swan”, such long-tailed distributions played a critical role in the 2008 financial crisis, because many of the financial models used by traders assumed that financial systems would follow the normal distribution, which they clearly did not.\n\n\n\n\nHaving read this chapter, you should be able to:\n\nCompute absolute, relative, and cumulative frequency distributions for a given dataset\nGenerate a graphical representation of a frequency distribution\nDescribe the difference between a normal and a long-tailed distribution, and describe the situations that commonly give rise to each\n\n\n\n\n\nThe Black Swan: The Impact of the Highly Improbable, by Nassim Nicholas Taleb",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#why-summarize-data",
    "href": "module-1.html#why-summarize-data",
    "title": "Summarizing data",
    "section": "",
    "text": "When we summarize data, we are necessarily throwing away information, and one might plausibly object to this. As an example, let’s go back to the PURE study that we discussed in Chapter 1. Are we not supposed to believe that all of the details about each individual matter, beyond those that are summarized in the dataset? What about the specific details of how the data were collected, such as the time of day or the mood of the participant? All of these details are lost when we summarize the data.\nOne reason that we summarize data is that it provides us with a way to generalize - that is, to make general statements that extend beyond specific observations. The importance of generalization was highlighted by the writer Jorge Luis Borges in his short story “Funes the Memorious”, which describes an individual who loses the ability to forget. Borges focuses in on the relation between generalization (i.e. throwing away data) and thinking: “To think is to forget a difference, to generalize, to abstract. In the overly replete world of Funes, there were nothing but details.”\nPsychologists have long studied all of the ways in which generalization is central to thinking. One example is categorization: We are able to easily recognize different examples of the category of “birds” even though the individual examples may be very different in their surface features (such as an ostrich, a robin, and a chicken). Importantly, generalization lets us make predictions about these individuals – in the case of birds, we can predict that they can fly and eat seeds, and that they probably can’t drive a car or speak English. These predictions won’t always be right, but they are often good enough to be useful in the world.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#summarizing-data-using-tables",
    "href": "module-1.html#summarizing-data-using-tables",
    "title": "Summarizing data",
    "section": "",
    "text": "A simple way to summarize data is to generate a table representing counts of various types of observations. This type of table has been used for thousands of years (see Figure @ref(fig:salesContract)).\n\n\n\n\n\nA Sumerian tablet from the Louvre, showing a sales contract for a house and field. Public domain, via Wikimedia Commons.\n\n\n\n\nLet’s look at some examples of the use of tables, using a more realistic dataset. Throughout this book we will use the National Health and Nutrition Examination Survey (NHANES) dataset. This is an ongoing study that assesses the health and nutrition status of a sample of individuals from the United States on many different variables. We will use a version of the dataset that is available for the R statistical software package. For this example, we will look at a simple variable, called PhysActive in the dataset. This variable contains one of three different values: “Yes” or “No” (indicating whether or not the person reports doing “moderate or vigorous-intensity sports, fitness or recreational activities”), or “NA” if the data are missing for that individual. There are different reasons that the data might be missing; for example, this question was not asked of children younger than 12 years of age, while in other cases an adult may have declined to answer the question during the interview, or the interviewer’s recording of the answer on their form might be unreadable.\n\n\nA distribution describes how data are divided between different possible values. For this example, let’s look at how many people fall into each of the physical activity categories.\n\n\n\nFrequency distribution for PhysActive variable\n\n\nPhysActive\nAbsoluteFrequency\n\n\n\n\nNo\n2473\n\n\nYes\n2972\n\n\nNA\n1334\n\n\n\n\n\nTable @ref(tab:PhysActiveTable) shows the frequencies of each of the different values; there were 2473 individuals who responded “No” to the question, 2972 who responded “Yes”, and 1334 for whom no response was given. We call this a frequency distribution because it tells us how frequent each of the possible values is within our sample.\nThis shows us the absolute frequency of the two responses, for everyone who actually gave a response. We can see from this that there are more people saying “Yes” than “No”, but it can be hard to tell from absolute numbers how big the difference is in relative terms. For this reason, we often would rather present the data using relative frequency, which is obtained by dividing each frequency by the sum of all frequencies:\n\\[\nrelative\\ frequency_i = \\frac{absolute\\ frequency_i}{\\sum_{j=1}^N absolute\\ frequency_j}\n\\] The relative frequency provides a much easier way to see how big the imbalance is. We can also interpret the relative frequencies as percentages by multiplying them by 100. In this example, we will drop the NA values as well, since we would like to be able to interpret the relative frequencies of active versus inactive people. However, for this to make sense we have to assume that the NA values are missing “at random”, meaning that their presence or absence is not related to the true value of the variable for that person. For example, if inactive participants were more likely to refuse to answer the question than active participants, then that would bias our estimate of the frequency of physical activity, meaning that our estimate would be different from the true value.\n\n\n\nAbsolute and relative frequencies and percentages for PhysActive variable\n\n\nPhysActive\nAbsoluteFrequency\nRelativeFrequency\nPercentage\n\n\n\n\nNo\n2473\n0.45\n45\n\n\nYes\n2972\n0.55\n55\n\n\n\n\n\nTable @ref(tab:PhysActiveTableFiltered) lets us see that 45.4 percent of the individuals in the NHANES sample said “No” and 54.6 percent said “Yes”.\n\n\n\nThe PhysActive variable that we examined above only had two possible values, but often we wish to summarize data that can have many more possible values. When those values are quantitative, then one useful way to summarize them is via what we call a cumulative frequency representation: rather than asking how many observations take on a specific value, we ask how many have a value some specific value or less.\nLet’s look at another variable in the NHANES dataset, called SleepHrsNight which records how many hours the participant reports sleeping on usual weekdays. Table @ref(tab:sleepTable) shows a frequency table created as we did above, after removing anyone with missing data for this question. We can already begin to summarize the dataset just by looking at the table; for example, we can see that most people report sleeping between 6 and 8 hours. To see this even more clearly, we can plot a histogram which shows the number of cases having each of the different values; see left panel of Figure @ref(fig:sleepHist). We can also plot the relative frequencies, which we will often refer to as densities - see the right panel of Figure @ref(fig:sleepHist).\n\n\n\nFrequency distribution for number of hours of sleep per night in the NHANES dataset\n\n\nSleepHrsNight\nAbsoluteFrequency\nRelativeFrequency\nPercentage\n\n\n\n\n2\n9\n0.00\n0.18\n\n\n3\n49\n0.01\n0.97\n\n\n4\n200\n0.04\n3.97\n\n\n5\n406\n0.08\n8.06\n\n\n6\n1172\n0.23\n23.28\n\n\n7\n1394\n0.28\n27.69\n\n\n8\n1405\n0.28\n27.90\n\n\n9\n271\n0.05\n5.38\n\n\n10\n97\n0.02\n1.93\n\n\n11\n15\n0.00\n0.30\n\n\n12\n17\n0.00\n0.34\n\n\n\n\n\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nLeft: Histogram showing the number (left) and proportion (right) of people reporting each possible value of the SleepHrsNight variable.\n\n\n\n\nWhat if we want to know how many people report sleeping 5 hours or less? To find this, we can compute a cumulative distribution. To compute the cumulative frequency for some value j, we add up the frequencies for all of the values up to and including j:\n\\[\ncumulative\\ frequency_j = \\sum_{i=1}^{j}{absolute\\ frequency_i}\n\\]\n\n\n\n\nAbsolute and cumulative frequency distributions for SleepHrsNight variable\n\n\nSleepHrsNight\nAbsoluteFrequency\nCumulativeFrequency\n\n\n\n\n2\n9\n9\n\n\n3\n49\n58\n\n\n4\n200\n258\n\n\n5\n406\n664\n\n\n6\n1172\n1836\n\n\n7\n1394\n3230\n\n\n8\n1405\n4635\n\n\n9\n271\n4906\n\n\n10\n97\n5003\n\n\n11\n15\n5018\n\n\n12\n17\n5035\n\n\n\n\n\nLet’s do this for our sleep variable, computing the absolute and cumulative frequency. In the left panel of Figure @ref(fig:sleepAbsCumulRelFreq) we plot the data to see what these representations look like; the absolute frequency values are plotted in solid lines, and the cumulative frequencies are plotted in dashed lines We see that the cumulative frequency is monotonically increasing – that is, it can only go up or stay constant, but it can never decrease. Again, we usually find the relative frequencies to be more useful than the absolute; those are plotted in the right panel of Figure @ref(fig:sleepAbsCumulRelFreq). Importantly, the shape of the relative frequency plot is exactly the same as the absolute frequency plot – only the size of the values has changed.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nA plot of the relative (solid) and cumulative relative (dashed) values for frequency (left) and proportion (right) for the possible values of SleepHrsNight.\n\n\n\n\n\n\n\n\n\n\n\n\nA histogram of the Age (left) and Height (right) variables in NHANES.\n\n\n\n\nThe variables that we examined above were fairly simple, having only a few possible values. Now let’s look at a more complex variable: Age. First let’s plot the Age variable for all of the individuals in the NHANES dataset (see left panel of Figure @ref(fig:ageHist)). What do you see there? First, you should notice that the number of individuals in each age group is declining over time. This makes sense because the population is being randomly sampled, and thus death over time leads to fewer people in the older age ranges. Second, you probably notice a large spike in the graph at age 80. What do you think that’s about?\nIf were were to look up the information about the NHANES dataset, we would see the following definition for the Age variable: “Age in years at screening of study participant. Note: Subjects 80 years or older were recorded as 80.” The reason for this is that the relatively small number of individuals with very high ages would make it potentially easier to identify the specific person in the dataset if you knew their exact age; researchers generally promise their participants to keep their identity confidential, and this is one of the things they can do to help protect their research subjects. This also highlights the fact that it’s always important to know where one’s data have come from and how they have been processed; otherwise we might interpret them improperly, thinking that 80-year-olds had been somehow overrepresented in the sample.\nLet’s look at another more complex variable in the NHANES dataset: Height. The histogram of height values is plotted in the right panel of Figure @ref(fig:ageHist). The first thing you should notice about this distribution is that most of its density is centered around about 170 cm, but the distribution has a “tail” on the left; there are a small number of individuals with much smaller heights. What do you think is going on here?\nYou may have intuited that the small heights are coming from the children in the dataset. One way to examine this is to plot the histogram with separate colors for children and adults (left panel of Figure @ref(fig:heightHistSep)). This shows that all of the very short heights were indeed coming from children in the sample. Let’s create a new version of NHANES that only includes adults, and then plot the histogram just for them (right panel of Figure @ref(fig:heightHistSep)). In that plot the distribution looks much more symmetric. As we will see later, this is a nice example of a normal (or Gaussian) distribution.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\nHistogram of heights for NHANES. A: values plotted separately for children (gray) and adults (black). B: values for adults only. C: Same as B, but with bin width = 0.1\n\n\n\n\n\n\n\nIn our earlier example with the sleep variable, the data were reported in whole numbers, and we simply counted the number of people who reported each possible value. However, if you look at a few values of the Height variable in NHANES (as shown in Table @ref(tab:heightVals)), you will see that it was measured in centimeters down to the first decimal place.\n\n\n\nA few values of Height from the NHANES data frame.\n\n\nHeight\n\n\n\n\n169.6\n\n\n169.8\n\n\n167.5\n\n\n155.2\n\n\n173.8\n\n\n174.5\n\n\n\n\n\nPanel C of Figure @ref(fig:heightHistSep) shows a histogram that counts the density of each possible value down the first decimal place. That histogram looks really jagged, which is because of the variability in specific decimal place values. For example, the value 173.2 occurs 32 times, while the value 173.3 only occurs 15 times. We probably don’t think that there is really such a big difference between the prevalence of these two heights; more likely this is just due to random variability in our sample of people.\nIn general, when we create a histogram of data that are continuous or where there are many possible values, we will bin the values so that instead of counting and plotting the frequency of every specific value, we count and plot the frequency of values falling within specific ranges. That’s why the plot looked less jagged above in Panel B of @ref(fig:heightHistSep); in this panel we set the bin width to 1, which means that the histogram is computed by combining values within bins with a width of one; thus, the values 1.3, 1.5, and 1.6 would all count toward the frequency of the same bin, which would span from values equal to one up through values less than 2.\nNote that once the bin size has been selected, then the number of bins is determined by the data:\n\\[\nnumber\\, of\\, bins  = \\frac{range\\, of\\, scores}{bin\\, width}\n\\]\nThere is no hard and fast rule for how to choose the optimal bin width. Occasionally it will be obvious (as when there are only a few possible values), but in many cases it would require trial and error. There are methods that try to find an optimal bin size automatically, such as the Freedman-Diaconis method that we will use in some later examples.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#idealized-representations-of-distributions",
    "href": "module-1.html#idealized-representations-of-distributions",
    "title": "Summarizing data",
    "section": "",
    "text": "Datasets are like snowflakes, in that every one is different, but nonetheless there are patterns that one often sees in different types of data. This allows us to use idealized representations of the data to further summarize them. Let’s take the adult height data plotted in @ref(fig:heightHistSep), and plot them alongside a very different variable: pulse rate (heartbeats per minute), also measured in NHANES (see Figure @ref(fig:NormalDistPlotsWithDist)).\n\n\n\n\n\nHistograms for height (left) and pulse (right) in the NHANES dataset, with the normal distribution overlaid for each dataset.\n\n\n\n\nWhile these plots certainly don’t look exactly the same, both have the general characteristic of being relatively symmetric around a rounded peak in the middle. This shape is in fact one of the commonly observed shapes of distributions when we collect data, which we call the normal (or Gaussian) distribution. This distribution is defined in terms of two values (which we call parameters of the distribution): the location of the center peak (which we call the mean) and the width of the distribution (which is described in terms of a parameter called the standard deviation). Figure @ref(fig:NormalDistPlotsWithDist) shows the appropriate normal distribution plotted on top of each of the histrograms.You can see that although the curves don’t fit the data exactly, they do a pretty good job of characterizing the distribution – with just two numbers!\nAs we will see later when we discuss the central limit theorem, there is a deep mathematical reason why many variables in the world exhibit the form of a normal distribution.\n\n\nThe examples in Figure @ref(fig:NormalDistPlotsWithDist) followed the normal distribution fairly well, but in many cases the data will deviate in a systematic way from the normal distribution. One way in which the data can deviate is when they are asymmetric, such that one tail of the distribution is more dense than the other. We refer to this as “skewness”. Skewness commonly occurs when the measurement is constrained to be non-negative, such as when we are counting things or measuring elapsed times (and thus the variable can’t take on negative values).\nAn example of relatively mild skewness can be seen in the average waiting times at the airport security lines at San Francisco International Airport, plotted in the left panel of Figure @ref(fig:SFOWaitTimes). You can see that while most wait times are less than 20 minutes, there are a number of cases where they are much longer, over 60 minutes! This is an example of a “right-skewed” distribution, where the right tail is longer than the left; these are common when looking at counts or measured times, which can’t be less than zero. It’s less common to see “left-skewed” distributions, but they can occur, for example when looking at fractional values that can’t take a value greater than one.\n\n\n\n\n\nExamples of right-skewed and long-tailed distributions. Left: Average wait times for security at SFO Terminal A (Jan-Oct 2017), obtained from https://awt.cbp.gov/ . Right: A histogram of the number of Facebook friends amongst 3,663 individuals, obtained from the Stanford Large Network Database. The person with the maximum number of friends is indicated by the diamond.\n\n\n\n\n\n\n\nHistorically, statistics has focused heavily on data that are normally distributed, but there are many data types that look nothing like the normal distribution. In particular, many real-world distributions are “long-tailed”, meaning that the right tail extends far beyond the most typical members of the distribution; that is, they are extremely skewed. One of the most interesting types of data where long-tailed distributions occur arises from the analysis of social networks. For an example, let’s look at the Facebook friend data from the Stanford Large Network Database and plot the histogram of number of friends across the 3,663 people in the database (see right panel of Figure @ref(fig:SFOWaitTimes)). As we can see, this distribution has a very long right tail – the average person has 24.09 friends, while the person with the most friends (denoted by the blue dot) has 1043!\nLong-tailed distributions are increasingly being recognized in the real world. In particular, many features of complex systems are characterized by these distributions, from the frequency of words in text, to the number of flights in and out of different airports, to the connectivity of brain networks. There are a number of different ways that long-tailed distributions can come about, but a common one occurs in cases of the so-called “Matthew effect” from the Christian Bible:\n\nFor to every one who has will more be given, and he will have abundance; but from him who has not, even what he has will be taken away. — Matthew 25:29, Revised Standard Version\n\nThis is often paraphrased as “the rich get richer”. In these situations, advantages compound, such that those with more friends have access to even more new friends, and those with more money have the ability to do things that increase their riches even more.\nAs the course progresses we will see several examples of long-tailed distributions, and we should keep in mind that many of the tools in statistics can fail when faced with long-tailed data. As Nassim Nicholas Taleb pointed out in his book “The Black Swan”, such long-tailed distributions played a critical role in the 2008 financial crisis, because many of the financial models used by traders assumed that financial systems would follow the normal distribution, which they clearly did not.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#learning-objectives",
    "href": "module-1.html#learning-objectives",
    "title": "Summarizing data",
    "section": "",
    "text": "Having read this chapter, you should be able to:\n\nCompute absolute, relative, and cumulative frequency distributions for a given dataset\nGenerate a graphical representation of a frequency distribution\nDescribe the difference between a normal and a long-tailed distribution, and describe the situations that commonly give rise to each",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#suggested-readings",
    "href": "module-1.html#suggested-readings",
    "title": "Summarizing data",
    "section": "",
    "text": "The Black Swan: The Impact of the Highly Improbable, by Nassim Nicholas Taleb",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#anatomy-of-a-plot",
    "href": "module-1.html#anatomy-of-a-plot",
    "title": "Summarizing data",
    "section": "Anatomy of a plot",
    "text": "Anatomy of a plot\nThe goal of plotting data is to present a summary of a dataset in a two-dimensional (or occasionally three-dimensional) presentation. We refer to the dimensions as axes – the horizontal axis is called the X-axis and the vertical axis is called the Y-axis. We can arrange the data along the axes in a way that highlights the data values. These values may be either continuous or categorical.\nThere are many different types of plots that we can use, which have different advantages and disadvantages. Let’s say that we are interested in characterizing the difference in height between men and women in the NHANES dataset. Figure @ref(fig:plotHeight) shows four different ways to plot these data.\n\nThe bar graph in panel A shows the difference in means, but doesn’t show us how much spread there is in the data around these means – and as we will see later, knowing this is essential to determine whether we think the difference between the groups is large enough to be important.\n\nThe second plot shows the bars with all of the data points overlaid - this makes it a bit clearer that the distributions of height for men and women are overlapping, but it’s still hard to see due to the large number of data points.\n\nIn general we prefer using a plotting technique that provides a clearer view of the distribution of the data points.\n\nIn panel C, we see one example of a violin plot, which plots the distribution of data in each condition (after smoothing it out a bit).\n\nAnother option is the box plot shown in panel D, which shows the median (central line), a measure of variability (the width of the box, which is based on a measure called the interquartile range), and any outliers (noted by the points at the ends of the lines). These are both effective ways to show data that provide a good feel for the distribution of the data.\n\n\n\n\n\n\nFour different ways of plotting the difference in height between men and women in the NHANES dataset. Panel A plots the means of the two groups, which gives no way to assess the relative overlap of the two distributions. Panel B shows the same bars, but also overlays the data points, jittering them so that we can see their overall distribution. Panel C shows a violin plot, which shows the distribution of the datasets for each group. Panel D shows a box plot, which highlights the spread of the distribution along with any outliers (which are shown as individual points).",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#principles-of-good-visualization",
    "href": "module-1.html#principles-of-good-visualization",
    "title": "Summarizing data",
    "section": "Principles of good visualization",
    "text": "Principles of good visualization\nMany books have been written on effective visualization of data. There are some principles that most of these authors agree on, while others are more contentious. Here we summarize some of the major principles; if you want to learn more, then some good resources are listed in the Suggested Readings section at the end of this chapter.\n\nShow the data and make them stand out\nLet’s say that I performed a study that examined the relationship between dental health and time spent flossing, and I would like to visualize my data. Figure @ref(fig:dentalFigs) shows four possible presentations of these data.\n\nIn panel A, we don’t actually show the data, just a line expressing the relationship between the data. This is clearly not optimal, because we can’t actually see what the underlying data look like.\n\nPanels B-D show three possible outcomes from plotting the actual data, where each plot shows a different way that the data might have looked.\n\nIf we saw the plot in Panel B, we would probably be suspicious – rarely would real data follow such a precise pattern.\n\nThe data in Panel C, on the other hand, look like real data – they show a general trend, but they are messy, as data in the world usually are.\n\nThe data in Panel D show us that the apparent relationship between the two variables is solely caused by one individual, who we would refer to as an outlier because they fall so far outside of the pattern of the rest of the group. It should be clear that we probably don’t want to conclude very much from an effect that is driven by one data point. This figure highlights why it is always important to look at the raw data before putting too much faith in any summary of the data.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFour different possible presentations of data for the dental health example. Each point in the scatter plot represents one data point in the dataset, and the line in each plot represents the linear trend in the data.\n\n\n\n\n\n\nMaximize the data/ink ratio\nEdward Tufte has proposed an idea called the data/ink ratio:\n\\[\ndata/ink\\ ratio = \\frac{amount\\, of\\, ink\\, used\\, on\\, data}{total\\, amount\\, of\\, ink}\n\\] The point of this is to minimize visual clutter and let the data show through. For example, take the two presentations of the dental health data in Figure @ref(fig:dataInkExample). Both panels show the same data, but panel A is much easier to apprehend, because of its relatively higher data/ink ratio.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range (`stat_smooth()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAn example of the same data plotted with two different data/ink ratios.\n\n\n\n\n\n\nAvoid chartjunk\nIt’s especially common to see presentations of data in the popular media that are adorned with lots of visual elements that are thematically related to the content but unrelated to the actual data. This is known as chartjunk, and should be avoided at all costs.\nOne good way to avoid chartjunk is to avoid using popular spreadsheet programs to plot one’s data. For example, the chart in Figure @ref(fig:chartJunk) (created using Microsoft Excel) plots the relative popularity of different religions in the United States. There are at least three things wrong with this figure:\n\nit has graphics overlaid on each of the bars that have nothing to do with the actual data\nit has a distracting background texture\nit uses three-dimensional bars, which distort the data\n\n\n\n\n\n\nAn example of chart junk.\n\n\n\n\n\n\nAvoid distorting the data\nIt’s often possible to use visualization to distort the message of a dataset. A very common one is use of different axis scaling to either exaggerate or hide a pattern of data. For example, let’s say that we are interested in seeing whether rates of violent crime have changed in the US. In Figure @ref(fig:crimePlotAxes), we can see these data plotted in ways that either make it look like crime has remained constant, or that it has plummeted. The same data can tell two very different stories!\n\n\n\n\n\nCrime data from 1990 to 2014 plotted over time. Panels A and B show the same data, but with different ranges of values along the Y axis. Data obtained from https://www.ucrdatatool.gov/Search/Crime/State/RunCrimeStatebyState.cfm\n\n\n\n\nOne of the major controversies in statistical data visualization is how to choose the Y axis, and in particular whether it should always include zero. In his famous book “How to lie with statistics”, Darrell Huff argued strongly that one should always include the zero point in the Y axis. On the other hand, Edward Tufte has argued against this:\n\n“In general, in a time-series, use a baseline that shows the data not the zero point; don’t spend a lot of empty vertical space trying to reach down to the zero point at the cost of hiding what is going on in the data line itself.” (from https://qz.com/418083/its-ok-not-to-start-your-y-axis-at-zero/)\n\nThere are certainly cases where using the zero point makes no sense at all. Let’s say that we are interested in plotting body temperature for an individual over time. In Figure @ref(fig:bodyTempAxis) we plot the same (simulated) data with or without zero in the Y axis. It should be obvious that by plotting these data with zero in the Y axis (Panel A) we are wasting a lot of space in the figure, given that body temperature of a living person could never go to zero! By including zero, we are also making the apparent jump in temperature during days 21-30 much less evident. In general, my inclination for line plots and scatterplots is to use all of the space in the graph, unless the zero point is truly important to highlight.\n\n\n\n\n\nBody temperature over time, plotted with or without the zero point in the Y axis.\n\n\n\n\nEdward Tufte introduced the concept of the lie factor to describe the degree to which physical differences in a visualization correspond to the magnitude of the differences in the data. If a graphic has a lie factor near 1, then it is appropriately representing the data, whereas lie factors far from one reflect a distortion of the underlying data.\nThe lie factor supports the argument that one should always include the zero point in a bar chart in many cases. In Figure @ref(fig:barCharLieFactor) we plot the same data with and without zero in the Y axis. In panel A, the proportional difference in area between the two bars is exactly the same as the proportional difference between the values (i.e. lie factor = 1), whereas in Panel B (where zero is not included) the proportional difference in area between the two bars is roughly 2.8 times bigger than the proportional difference in the values, and thus it visually exaggerates the size of the difference.\n\n\n\n\n\nTwo bar charts with associated lie factors.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#accommodating-human-limitations",
    "href": "module-1.html#accommodating-human-limitations",
    "title": "Summarizing data",
    "section": "Accommodating human limitations",
    "text": "Accommodating human limitations\nHumans have both perceptual and cognitive limitations that can make some visualizations very difficult to understand. It’s always important to keep these in mind when building a visualization.\n\nPerceptual limitations\nOne important perceptual limitation that many people (including myself) suffer from is color blindness. This can make it very difficult to perceive the information in a figure (like the one in Figure @ref(fig:badColors)) where there is only color contrast between the elements but no brightness contrast. It is always helpful to use graph elements that differ substantially in brightness and/or texture, in addition to color. There are also “colorblind-friendly” palettes available for use with many visualization tools.\n\n\n\n\n\nExample of a bad figure that relies solely on color contrast.\n\n\n\n\nEven for people with perfect color vision, there are perceptual limitations that can make some plots ineffective. This is one reason why statisticians never use pie charts: It can be very difficult for humans to accurately perceive differences in the volume of shapes. The pie chart in Figure @ref(fig:pieChart) (presenting the same data on religious affiliation that we showed above) shows how tricky this can be.\n\n\n\n\n\nAn example of a pie chart, highlighting the difficulty in apprehending the relative volume of the different pie slices.\n\n\n\n\nThis plot is terrible for several reasons. First, it requires distinguishing a large number of colors from very small patches at the bottom of the figure. Second, the visual perspective distorts the relative numbers, such that the pie wedge for Catholic appears much larger than the pie wedge for None, when in fact the number for None is slightly larger (22.8 vs 20.8 percent), as was evident in Figure @ref(fig:chartJunk). Third, by separating the legend from the graphic, it requires the viewer to hold information in their working memory in order to map between the graphic and legend and to conduct many “table look-ups” in order to continuously match the legend labels to the visualization. And finally, it uses text that is far too small, making it impossible to read without zooming in.\nPlotting the data using a more reasonable approach (Figure @ref(fig:religionBars)), we can see the pattern much more clearly. This plot may not look as flashy as the pie chart generated using Excel, but it’s a much more effective and accurate representation of the data.\n\n\n\n\n\nA clearer presentation of the religious affiliation data (obtained from http://www.pewforum.org/religious-landscape-study/).\n\n\n\n\nThis plot allows the viewer to make comparisons based on the the length of the bars along a common scale (the y-axis). Humans tend to be more accurate when decoding differences based on these perceptual elements than based on area or color.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#correcting-for-other-factors",
    "href": "module-1.html#correcting-for-other-factors",
    "title": "Summarizing data",
    "section": "Correcting for other factors",
    "text": "Correcting for other factors\nOften we are interested in plotting data where the variable of interest is affected by other factors than the one we are interested in. For example, let’s say that we want to understand how the price of gasoline has changed over time. Figure @ref(fig:gasPrices) shows historical gas price data, plotted either with or without adjustment for inflation. Whereas the unadjusted data show a huge increase, the adjusted data show that this is mostly just reflective of inflation. Other examples where one needs to adjust data for other factors include population size and data collected across different seasons.\n\n\n\n\n\nThe price of gasoline in the US from 1930 to 2013 (obtained from http://www.thepeoplehistory.com/70yearsofpricechange.html) with or without correction for inflation (based on Consumer Price Index).",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#learning-objectives-1",
    "href": "module-1.html#learning-objectives-1",
    "title": "Summarizing data",
    "section": "Learning objectives",
    "text": "Learning objectives\nHaving read this chapter, you should be able to:\n\nDescribe the principles that distinguish between good and bad graphs, and use them to identify good versus bad graphs.\nUnderstand the human limitations that must be accommodated in order to make effective graphs.\nPromise to never create a pie chart. Ever.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-1.html#suggested-readings-and-videos",
    "href": "module-1.html#suggested-readings-and-videos",
    "title": "Summarizing data",
    "section": "Suggested readings and videos",
    "text": "Suggested readings and videos\n\nFundamentals of Data Visualization, by Claus Wilke\nVisual Explanations, by Edward Tufte\nVisualizing Data, by William S. Cleveland\nGraph Design for the Eye and Mind, by Stephen M. Kosslyn\nHow Humans See Data, by John Rauser",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "module-3.html",
    "href": "module-3.html",
    "title": "Module 3",
    "section": "",
    "text": "Module 3",
    "crumbs": [
      "Modules",
      "Module 3: Estimation and sampling",
      "Module 3"
    ]
  },
  {
    "objectID": "module-5.html",
    "href": "module-5.html",
    "title": "Module 5",
    "section": "",
    "text": "Module 5",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Module 5"
    ]
  },
  {
    "objectID": "pythonbasics.html",
    "href": "pythonbasics.html",
    "title": "Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Python basics",
      "Python"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]